Using username "pi".
Authenticating with public key "rsa-key-20211122"
Linux node01 5.10.63-v7+ #1459 SMP Wed Oct 6 16:41:10 BST 2021 armv7l

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Tue Dec 14 18:05:25 2021 from 192.168.1.254

Wi-Fi is currently blocked by rfkill.
Use raspi-config to set the country before use.

pi@node01:~ $ g++ --version
g++ (Raspbian 10.2.1-6+rpi1) 10.2.1 20210110
Copyright (C) 2020 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

pi@node01:~ $ cd /clusterfs/
pi@node01:/clusterfs $ nano lab4_sub.sh
pi@node01:/clusterfs $ srun --pty bash
pi@node02:/clusterfs $ mpicxx lab4.cpp
g++: error: lab4.cpp: No such file or directory
pi@node02:/clusterfs $ mpicxx lab7.cpp
lab7.cpp: In function ‘int main(int, char**)’:
lab7.cpp:23:41: error: expected ‘;’ before ‘scanf’
   23 |         printf("Type the array size \n")
      |                                         ^
      |                                         ;
   24 |         scanf("%i %i", &m, &n);
      |         ~~~~~
pi@node02:/clusterfs $ nano lab7.cpp
pi@node02:/clusterfs $ mpicxx lab7.cpp
pi@node02:/clusterfs $ ls
a.out                             helloworld.txt  normal         slurm-82.out
calc-pi                           lab4_sub.sh     slurm-228.out  slurm.conf
cgroup_allowed_devices_file.conf  lab7.cpp        slurm-230.out  sub_mpi.sh
cgroup.conf                       lost+found      slurm-60.out   usr
hello_mpi.c                       mike.txt        slurm-6.out
helloworld.sh                     munge.key       slurm-73.out
pi@node02:/clusterfs $ nano lab4_sub.sh
pi@node02:/clusterfs $ exit
exit
pi@node01:/clusterfs $ sbatch lab4_sub.sh
Submitted batch job 238
pi@node01:/clusterfs $ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REA                                                                                                             SON)
               238 mycluster lab4_sub       pi  R       0:00      3 node[02-04]
pi@node01:/clusterfs $ cat slurm-238.out
Master node: node02
Type the array size
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 0 on node node02 exited on signal 11                                                                                                              (Segmentation fault).
--------------------------------------------------------------------------
2 total processes killed (some possibly by mpirun during cleanup)
pi@node01:/clusterfs $ nano lab7.cpp
pi@node01:/clusterfs $ srun --pty bash
pi@node02:/clusterfs $ mpicxx lab7.cpp
lab7.cpp: In function ‘int main(int, char**)’:
lab7.cpp:23:2: error: lvalue required as left operand of assignment
   23 |  &m = 4;
      |  ^~
lab7.cpp:24:2: error: lvalue required as left operand of assignment
   24 |  &n = 5;
      |  ^~
pi@node02:/clusterfs $ nano lab7.cpp
pi@node02:/clusterfs $ mpicxx lab7.cpp
pi@node02:/clusterfs $ ls
a.out                             helloworld.txt  normal         slurm-73.out
calc-pi                           lab4_sub.sh     slurm-228.out  slurm-82.out
cgroup_allowed_devices_file.conf  lab7.cpp        slurm-230.out  slurm.conf
cgroup.conf                       lost+found      slurm-238.out  sub_mpi.sh
hello_mpi.c                       mike.txt        slurm-60.out   usr
helloworld.sh                     munge.key       slurm-6.out
pi@node02:/clusterfs $ exit
exit
pi@node01:/clusterfs $ sbatch lab4_sub.sh
Submitted batch job 240
pi@node01:/clusterfs $ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REA                                                                                                             SON)
               240 mycluster lab4_sub       pi  R       0:01      3 node[02-04]
pi@node01:/clusterfs $ cat slurm-240.out
Master node: node02
Hi, I'm 0 My sum is: 322
Hi, I'm 1 My sum is: 218
Hi, I'm 2 My sum is: 176
sumAll = 0
sumAll = 716
sumAll = 0
